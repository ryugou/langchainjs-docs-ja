---
title: チャットモデル
slug: /concepts/chat_models
sidebar_label: チャットモデル
tags: [langchainjs, concepts, chat models]
---

# チャットモデル

## 概要

大規模言語モデル（LLM）は、テキスト生成、翻訳、要約、質問応答など、幅広い言語関連タスクに優れ、各タスクごとの個別学習を行わずに活用できます。
近年の LLM は、多くがチャットモデルのインターフェイス（メッセージのリストを入力し、メッセージを出力）で提供されます。

最新世代のチャットモデルは次の追加機能を備えています。

- ツール呼び出し（Tool calling）：多くの人気モデルがネイティブのツール呼び出し API を提供します。これにより、外部サービス・API・データベースと連携するリッチなアプリを構築できます。非構造データからの構造化情報抽出などにも利用可能です。

- 構造化出力（Structured output）：与えたスキーマに合致する JSON など、特定の構造で応答させる手法。

- マルチモーダル（Multimodality）：テキスト以外（画像・音声・動画など）のデータも扱える能力。

## 機能

LangChain は、さまざまなプロバイダのチャットモデルを一貫したインターフェイスで扱えるようにし、加えて LLM アプリの監視・デバッグ・性能最適化のための機能を提供します。

- Anthropic / OpenAI / Ollama / Microsoft Azure / Google Vertex / Amazon Bedrock / Hugging Face / Cohere / Groq など、多数のプロバイダ統合（最新の対応一覧は chat model integrations を参照）。

- LangChain 独自のメッセージ形式 または OpenAI 形式を使用可能。

- 標準ツール呼び出し API：モデルへのツールバインド、モデルが要求したツールコールの受け取り、ツール結果のモデルへの返却を標準化。

- withStructuredOutput による標準的な構造化出力 API。

- LangSmith との統合により、本番運用レベルのアプリを監視・デバッグ。

- トークン使用量の標準化、レート制限、キャッシュなどの追加機能。

- インテグレーション

LangChain は多数のチャットモデル統合を提供し、幅広いモデルを利用できます。統合は次の 2 種です。

- 公式モデル（Official models）：LangChain および／またはモデル提供元が公式サポート。@langchain/&lt;provider&gt; パッケージに含まれます。

- コミュニティモデル（Community models）：主にコミュニティが貢献・サポート。@langchain/community に含まれます。

命名規則として、LangChain のチャットモデルはクラス名に 「Chat」 を前置します（例：ChatOllama, ChatAnthropic, ChatOpenAI など）。
対応モデル一覧は chat model integrations を参照してください。

:::note
名前に「Chat」を含まない、または末尾に「LLM」が付くモデルは、通常、チャットインターフェイスではなく「文字列入力 → 文字列出力」の旧来インターフェイスを指します。
:::

## インターフェイス

チャットモデルは BaseChatModel を実装します。BaseChatModel は Runnable インターフェイスも実装しているため、標準的なストリーミングや最適化されたバッチ処理などをサポートします（詳細は Runnable インターフェイスを参照）。

多くの主要メソッドは、メッセージを入力として受け取り、メッセージを出力として返します。
また、チャットモデルは設定可能な標準パラメータ（出力の温度、最大トークン数、タイムアウト等）を備えます（詳細は standard parameters 参照）。

:::note
ドキュメントの中で「LLM」と「チャットモデル」を同義で使う場合があります。多くの最新 LLM はチャットインターフェイスで公開されるためです。ただし、LangChain には旧来の LLM 実装（文字列入出力、BaseLLM を実装、...LLM の名称を持つ場合あり）もありますが、一般には推奨されません。
:::

### 主要メソッド（Key methods）

- invoke：チャットモデルとのやり取りの基本メソッド。メッセージ配列を入力し、メッセージ配列を出力します。

- stream：生成中の出力をストリームで受け取ります。

- batch：複数リクエストをまとめて処理し、効率化します。

- bindTools：モデルの実行コンテキストでツールをバインドします。

- withStructuredOutput：ネイティブ構造化出力対応モデルに対する invoke のラッパー。

その他は BaseChatModel API Reference を参照

### 入出力

モダンな LLM は、メッセージ（例：system / human / assistant）を入力・出力とし、各メッセージはテキストやマルチモーダル（画像・音声・動画など）のコンテンツブロックを 1 つ以上含み得ます。

LangChain は以下 2 つのメッセージ形式をサポートします。

- LangChain Message Format：LangChain 独自形式（既定・内部でも使用）。

- OpenAI Message Format：OpenAI 互換形式。

###標準パラメータ

多くのチャットモデルは、以下の標準化された設定を受け付けます。

| パラメータ                                                       | 説明                                                       |
| ---------------------------------------------------------------- | ---------------------------------------------------------- |
| model 使用するモデル名または識別子（例：gpt-3.5-turbo, gpt-4）。 |
| temperature                                                      | 出力のランダム性。大きいほど創造的、小さいほど決定的で集中 |
| timeout                                                          | 応答待ちの最大時間（秒）。ハング防止                       |
| maxTokens                                                        | 応答の最大トークン数。出力の長さを制御                     |
| stop                                                             | 生成停止のシーケンス（特定の文字列で出力を終了）           |
| maxRetries                                                       | タイムアウトやレート制限等で失敗した際の再試行回数         |
| apiKey                                                           | モデル提供元への認証に必要な API キー                      |
| baseUrl                                                          | リクエストを送る API エンドポイント URL                    |

注意事項：

- 標準パラメータは、同等の機能を公開しているプロバイダにのみ適用されます（例：最大出力トークンを設定できないプロバイダも存在）。

- 標準パラメータの強制は、@langchain/openai, @langchain/anthropic など専用パッケージの統合でのみ行われ、@langchain/community のモデルには適用されません。

各チャットモデル固有の追加パラメータもあります。当該モデルの API リファレンスで確認してください。

### ツール呼び出し

チャットモデルは、DB 参照、外部 API 呼び出し、カスタムコード実行などのタスクをツールを通じて行えます。詳細は tool calling guide を参照。

## 構造化出力

モデルに所定の形式（例：スキーマ適合 JSON）で回答させられます。情報抽出タスクで非常に有用です。詳細は structured outputs guide を参照。

## マルチモーダル

LLM はテキストに限られず、画像・音声・動画なども扱えます。ただし、入力のマルチモーダル対応は一部モデルのみ、出力のマルチモーダル対応はほぼ未対応です。詳細は各モデルのドキュメントを参照。

## コンテキストウィンドウ

チャットモデルが一度に処理できる入力長の上限を指します。近年は大きくなっているものの、依然として制約です。入力が上限を超えると全体を処理できずエラーになり得ます。特に会話アプリでは、どれだけ会話の履歴を保持できるかに直結するため、開発者はウィンドウ内に収まるよう履歴管理（メモリ管理）が必要です。詳細は memory を参照。
入力サイズはトークンで計測されます。

## 上級トピック

### キャッシュ

チャットモデル API は遅くなりがちなため、応答のキャッシュは自然な発想です。理論上、プロバイダへのリクエストを減らし性能改善に寄与しますが、実際には難しい問題であり慎重に扱うべきです。

厳密入力一致に基づくキャッシュでは、会話の 1 ～ 2 往復目を超えるとヒット確率が低下します（同一の最初のメッセージ列が繰り返される場合は稀）。
代替としてセマンティックキャッシュ（入力の意味に基づくキャッシュ）がありますが、埋め込みモデルなど別モデル依存が増え、意味を正確に捉えられない場合もあります。

一方で、FAQ 応答のように同種の質問が頻出する場面では、負荷軽減・応答時間短縮に有効な場合があります。
詳細は how to cache chat model responses を参照してください。

## 関連リソース

- チャットモデルの使い方

- 対応チャットモデル一覧

## コンセプトガイド

Messages / Tool calling / Multimodality / Structured outputs / Tokens
